{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬°Hola Natalia! üòä\n",
    "\n",
    "Mi nombre es **Alejandro Castellanos** y hoy tengo el placer de ser el revisor de tu proyecto.\n",
    "\n",
    "Voy a revisar todo tu c√≥digo con detalle, buscando tanto los puntos fuertes como aquellos en los que podr√≠as mejorar. Te dejar√© comentarios a lo largo del notebook, destacando lo que has hecho bien y sugiriendo ajustes donde sea necesario. Si encuentro alg√∫n error, no te preocupes, te lo har√© saber de forma clara y te dar√© informaci√≥n √∫til para que puedas corregirlo en la pr√≥xima iteraci√≥n. Si en alg√∫n punto tienes comentarios, si√©ntete libre de dejarlos tambi√©n.\n",
    "\n",
    "\n",
    "Encontrar√°s mis comentarios espec√≠ficos dentro de cajas verdes, amarillas o rojas, es muy importante que no muevas, modifiques o borres mis comentarios, con el fin de tener un seguimiento adecuado de tu proceso:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "Si todo est√° perfecto.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "Si tu c√≥digo est√° bien pero se puede mejorar o hay alg√∫n detalle que le hace falta.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Comentario del revisor</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "Si de pronto hace falta algo o existe alg√∫n problema con tu c√≥digo o conclusiones.\n",
    "</div>\n",
    "\n",
    "Puedes responderme de esta forma:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Respuesta del estudiante</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "</div>\n",
    "\n",
    "A continuaci√≥n te dejar√© un comentario general con mi valoraci√≥n del proyecto. **¬°Mi objetivo es que sigas aprendiendo y mejorando con cada paso!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario General del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Natalia quiero felicitarte por el excelente trabajo que hiciste. Has realizado un an√°lisis exploratorio de datos (EDA) apropiado, preparando la informaci√≥n de manera cuidadosa para el entrenamiento de los modelos predictivos, lo cual establece una base s√≥lida para las etapas posteriores. Tu implementaci√≥n de tus modelos de clasifiaci√≥n ha sido acertada en combinaci√≥n con t√©cnicas de manejo de clases desbalanceadas, logrando alcanzar los objetivos establecidos en t√©rminos del F1-Score, lo que demuestra tu capacidad para ajustar modelos de clasificaci√≥n efectivos.  \n",
    "    \n",
    "    \n",
    "Recuerda que en el aprendizaje autom√°tico, la exploraci√≥n de diferentes t√©cnicas y enfoques es clave para desarrollar una comprensi√≥n m√°s profunda y encontrar las soluciones m√°s efectivas para cada problema espec√≠fico. Ac√° te comparto t√©cnicas adicionales de manejo de equilibrio de clases que te pueden resultar √∫tiles en otros proyectos: [10 t√©cnicas para resolver clases desequilibradas en ML](https://www.analyticsvidhya.com/articles/class-imbalance-in-machine-learning/)\n",
    "    \n",
    "Te deseo √©xitos en tu pr√≥ximo sprint üöÄ\n",
    "    \n",
    "    \n",
    "*Estado del Proyecto:* **Aprobado**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto - Modelo para predecir salida de clientes de Beta Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1. Descarga y preparaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importaci√≥n inicial de librerias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0     2.0       0.00              1          1               1   \n",
      "1     1.0   83807.86              1          0               1   \n",
      "2     8.0  159660.80              3          1               0   \n",
      "3     1.0       0.00              2          0               0   \n",
      "4     2.0  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "         RowNumber    CustomerId   CreditScore           Age       Tenure  \\\n",
      "count  10000.00000  1.000000e+04  10000.000000  10000.000000  9091.000000   \n",
      "mean    5000.50000  1.569094e+07    650.528800     38.921800     4.997690   \n",
      "std     2886.89568  7.193619e+04     96.653299     10.487806     2.894723   \n",
      "min        1.00000  1.556570e+07    350.000000     18.000000     0.000000   \n",
      "25%     2500.75000  1.562853e+07    584.000000     32.000000     2.000000   \n",
      "50%     5000.50000  1.569074e+07    652.000000     37.000000     5.000000   \n",
      "75%     7500.25000  1.575323e+07    718.000000     44.000000     7.000000   \n",
      "max    10000.00000  1.581569e+07    850.000000     92.000000    10.000000   \n",
      "\n",
      "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
      "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
      "mean    76485.889288       1.530200      0.70550        0.515100   \n",
      "std     62397.405202       0.581654      0.45584        0.499797   \n",
      "min         0.000000       1.000000      0.00000        0.000000   \n",
      "25%         0.000000       1.000000      0.00000        0.000000   \n",
      "50%     97198.540000       1.000000      1.00000        1.000000   \n",
      "75%    127644.240000       2.000000      1.00000        1.000000   \n",
      "max    250898.090000       4.000000      1.00000        1.000000   \n",
      "\n",
      "       EstimatedSalary        Exited  \n",
      "count     10000.000000  10000.000000  \n",
      "mean     100090.239881      0.203700  \n",
      "std       57510.492818      0.402769  \n",
      "min          11.580000      0.000000  \n",
      "25%       51002.110000      0.000000  \n",
      "50%      100193.915000      0.000000  \n",
      "75%      149388.247500      0.000000  \n",
      "max      199992.480000      1.000000  \n",
      "RowNumber            0\n",
      "CustomerId           0\n",
      "Surname              0\n",
      "CreditScore          0\n",
      "Geography            0\n",
      "Gender               0\n",
      "Age                  0\n",
      "Tenure             909\n",
      "Balance              0\n",
      "NumOfProducts        0\n",
      "HasCrCard            0\n",
      "IsActiveMember       0\n",
      "EstimatedSalary      0\n",
      "Exited               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "# Exploraci√≥n inicial\n",
    "print(data.head())  # Primeras filas\n",
    "print(data.info())  # Informaci√≥n general\n",
    "print(data.describe())  # Estad√≠sticas descriptivas\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Muy buen trabajo importando las librer√≠as y los datos del proyecto. Adicionalmente usaste correctamente las funciones `info`, `describe`y `head`, esto te permite hacer una primera revisi√≥n de los datos, su estructura y contenido. Con esta informaci√≥n, podemos establecer una hoja de ruta para ajustar, modificar y analizar los datos de una manera adecuada.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore        0\n",
      "Geography          0\n",
      "Gender             0\n",
      "Age                0\n",
      "Tenure             0\n",
      "Balance            0\n",
      "NumOfProducts      0\n",
      "HasCrCard          0\n",
      "IsActiveMember     0\n",
      "EstimatedSalary    0\n",
      "Exited             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Eliminar columnas irrelevantes\n",
    "data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# Imputar valores nulos en la columna 'Tenure' con la mediana\n",
    "median_tenure = data['Tenure'].median()\n",
    "data['Tenure'].fillna(median_tenure, inplace=True)\n",
    "\n",
    "# Verificar que no queden valores nulos\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Codificar variables categ√≥ricas\n",
    "data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "# Dividir en caracter√≠sticas (X) y objetivo (y)\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validaci√≥n y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12345)\n",
    "\n",
    "# Estandarizar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Buen trabajo manejando los datas faltantes de¬†Tenure, esto es clave para evitar errores en el desarrollo de los modelos. Adicionalmente, eliminastes las columnas que no nos aportan al desarrollo de los modelos predictivos. Por otro lado, la codificaci√≥n de las variables categ√≥ricas y el escalado de las valores num√©ricos le da mayor robustez a los modelos y mejora su rendimiento ,\n",
    "    .\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Con respecto a la imputaci√≥n de datos faltantes, si bien usar la mediana es una opci√≥n v√°lida, hay otros m√©todos que pueden ser m√°s adecuados y que pueden reducir la posibilidad de agregar sesgo a los datos, ac√° te comparto un art√≠culo donde hablan sobre diferentes [m√©todos de imputaci√≥n](https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python)\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2. Examinar el equilibrio de clases y modelo b√°sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5589\n",
      "1    1411\n",
      "Name: Exited, dtype: int64\n",
      "0    0.798429\n",
      "1    0.201571\n",
      "Name: Exited, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Contar las clases\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Porcentaje de cada clase\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase 0 (No se fueron): 5,589 observaciones (79.84%).\n",
    "\n",
    "Clase 1 (Se fueron): 1,411 observaciones (20.16%).\n",
    "\n",
    "Esto confirma que hay un desequilibrio de clases, ya que la clase 0 es aproximadamente 4 veces m√°s frecuente que la clase 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Muy bien Natalia, has identificado que el dataset est√° desbalanceado. Esto es clave para definir el plan de trabajo\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Este tipo de an√°lisis es conveniente hacerlo antes de hacer la segmetnaci√≥n de los datos en entrenamiento, validaci√≥n y prueba. Adicionalmente, si quieres que esta segmentaci√≥n conserve la proporci√≥n de cada clase debes usar el par√°metro `stratify`\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=12345)\n",
    "\n",
    "Esto asegura que la proporci√≥n de clases en `y_train` y `y_temp` sea similar a la de `y` original.\n",
    "</div>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.319\n",
      "AUC-ROC: 0.776\n"
     ]
    }
   ],
   "source": [
    "#Modelo B√°sico\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Entrenar modelo de regresi√≥n log√≠stica\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "auc_roc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC-ROC: {auc_roc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un valor del F1 Score (0.319) sugiere que el modelo tiene dificultades para predecir correctamente la clase positiva (clientes que se van). Esto es com√∫n en conjuntos de datos desequilibrados, donde la clase minoritaria (en este caso, Exited = 1) no est√° bien representada.\n",
    "\n",
    "AUC-ROC (0.776): El AUC-ROC mide la capacidad del modelo para distinguir entre las clases positiva y negativa. Un valor de 0.776 indica que el modelo tiene un rendimiento decente en t√©rminos de separaci√≥n de clases, pero no es excelente.\n",
    "\n",
    "Aunque el AUC-ROC es relativamente bueno, el F1 Score bajo sugiere que el modelo no est√° equilibrando bien la precisi√≥n y el recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Excelente trabajo implemetando el modelo de regresi√≥n l√≥gistica sin ajuste de equilibrio de clases, esto permite tener una base para ver el efecto de las t√©cnicas empleadas\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Al importar librer√≠as en un notebook de Python, una buena pr√°ctica es incluir en la primera celda todas las que vas a emplear en el proyecto, organiz√°ndolas por orden de importancia o categor√≠as: primero las librer√≠as est√°ndar, seguidas de las de terceros y, finalmente, cualquier m√≥dulo personalizado. Adem√°s, documenta librer√≠as menos comunes con comentarios para facilitar su comprensi√≥n\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3.  Mejoras al modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Regresi√≥n logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o de X_train_downsampled: (1970, 11)\n",
      "Tama√±o de y_train_downsampled: (1970,)\n"
     ]
    }
   ],
   "source": [
    "#SUBMUESTREO\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def downsample(X, y, fraction):\n",
    "    # Asegurarse de que X e y tengan los mismos √≠ndices\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    \n",
    "    # Filtrar las clases\n",
    "    X_zeros = X[y == 0]\n",
    "    X_ones = X[y == 1]\n",
    "    y_zeros = y[y == 0]\n",
    "    y_ones = y[y == 1]\n",
    "\n",
    "    # Submuestrear la clase mayoritaria\n",
    "    X_zeros_downsampled = X_zeros.sample(frac=fraction, random_state=12345)\n",
    "    y_zeros_downsampled = y_zeros.sample(frac=fraction, random_state=12345)\n",
    "\n",
    "    # Combinar las clases\n",
    "    X_downsampled = pd.concat([X_zeros_downsampled, X_ones])\n",
    "    y_downsampled = pd.concat([y_zeros_downsampled, y_ones])\n",
    "\n",
    "    # Mezclar los datos\n",
    "    return shuffle(X_downsampled, y_downsampled, random_state=12345)\n",
    "\n",
    "# Aplicar submuestreo\n",
    "X_train_downsampled, y_train_downsampled = downsample(X_train, y_train, fraction=0.1)\n",
    "\n",
    "# Verificar el tama√±o de los datos submuestreados\n",
    "print(\"Tama√±o de X_train_downsampled:\", X_train_downsampled.shape)\n",
    "print(\"Tama√±o de y_train_downsampled:\", y_train_downsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Submuestreo): 0.412\n",
      "AUC-ROC (Submuestreo): 0.777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Entrenar el modelo con datos submuestreados\n",
    "model_downsampled = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_downsampled.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n\n",
    "y_pred_downsampled = model_downsampled.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_downsampled = f1_score(y_val, y_pred_downsampled)\n",
    "auc_roc_downsampled = roc_auc_score(y_val, model_downsampled.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Submuestreo): {f1_downsampled:.3f}')\n",
    "print(f'AUC-ROC (Submuestreo): {auc_roc_downsampled:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el m√©todo de submuestreo se observa un mejor rendimiento del modelo, una mejora en el F1 Score (0.41) y el AUC-ROC se mantuvo en 0.77. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de clases despu√©s de sobremuestreo:\n",
      "0    5589\n",
      "1    4233\n",
      "Name: Exited, dtype: int64\n",
      "F1 Score (Sobremuestreo): 0.512\n",
      "AUC-ROC (Sobremuestreo): 0.779\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Funci√≥n para sobremuestrear la clase minoritaria\n",
    "def oversample(X, y):\n",
    "    # Asegurarse de que X e y tengan los mismos √≠ndices\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    \n",
    "    # Filtrar las clases\n",
    "    X_zeros = X[y == 0]\n",
    "    X_ones = X[y == 1]\n",
    "    y_zeros = y[y == 0]\n",
    "    y_ones = y[y == 1]\n",
    "\n",
    "    # Calcular cu√°ntas veces se debe duplicar la clase minoritaria\n",
    "    n_zeros = len(X_zeros)\n",
    "    n_ones = len(X_ones)\n",
    "    repeat_times = n_zeros // n_ones  # N√∫mero de veces que se duplicar√° la clase minoritaria\n",
    "\n",
    "    # Duplicar la clase minoritaria\n",
    "    X_ones_oversampled = pd.concat([X_ones] * repeat_times)\n",
    "    y_ones_oversampled = pd.concat([y_ones] * repeat_times)\n",
    "\n",
    "    # Combinar las clases\n",
    "    X_oversampled = pd.concat([X_zeros, X_ones_oversampled])\n",
    "    y_oversampled = pd.concat([y_zeros, y_ones_oversampled])\n",
    "\n",
    "    # Mezclar los datos\n",
    "    return shuffle(X_oversampled, y_oversampled, random_state=12345)\n",
    "\n",
    "# Aplicar sobremuestreo\n",
    "X_train_oversampled, y_train_oversampled = oversample(X_train, y_train)\n",
    "\n",
    "# Verificar el nuevo tama√±o de los datos\n",
    "print(\"Distribuci√≥n de clases despu√©s de sobremuestreo:\")\n",
    "print(pd.Series(y_train_oversampled).value_counts())\n",
    "\n",
    "# Entrenar el modelo con datos sobremuestreados\n",
    "model_oversampled = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n\n",
    "y_pred_oversampled = model_oversampled.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_oversampled = f1_score(y_val, y_pred_oversampled)\n",
    "auc_roc_oversampled = roc_auc_score(y_val, model_oversampled.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Sobremuestreo): {f1_oversampled:.3f}')\n",
    "print(f'AUC-ROC (Sobremuestreo): {auc_roc_oversampled:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuci√≥n de Clases: El sobremuestreo ha aumentado el n√∫mero de instancias de la clase minoritaria (1) de 1,411 a 4,233.\n",
    "\n",
    "Aunque no se ha alcanzado un equilibrio perfecto (5,589 vs. 4,233), la clase minoritaria est√° mejor representada.\n",
    "\n",
    "F1 Score (0.512): El F1 Score ha mejorado en comparaci√≥n con el modelo b√°sico (0.319), lo que indica que el sobremuestreo ha ayudado a equilibrar mejor la precisi√≥n y el recall.\n",
    "\n",
    "Sin embargo, el valor a√∫n est√° por debajo del objetivo de 0.59.\n",
    "\n",
    "AUC-ROC (0.779): El AUC-ROC se mantiene relativamente alto, lo que sugiere que el modelo tiene una buena capacidad para distinguir entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ponderaci√≥n de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Ponderaci√≥n): 0.504\n",
      "AUC-ROC (Ponderaci√≥n): 0.780\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "# Preprocesamiento\n",
    "data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)  # Eliminar columnas irrelevantes\n",
    "data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)  # Codificar variables categ√≥ricas\n",
    "\n",
    "# Imputar valores nulos en 'Tenure'\n",
    "median_tenure = data['Tenure'].median()\n",
    "data['Tenure'].fillna(median_tenure, inplace=True)\n",
    "\n",
    "# Separar caracter√≠sticas (X) y objetivo (y)\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validaci√≥n y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12345)\n",
    "\n",
    "# Estandarizar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar el modelo con ponderaci√≥n de clases\n",
    "model_weighted = LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced')\n",
    "model_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n\n",
    "y_pred_weighted = model_weighted.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_weighted = f1_score(y_val, y_pred_weighted)\n",
    "auc_roc_weighted = roc_auc_score(y_val, model_weighted.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Ponderaci√≥n): {f1_weighted:.3f}')\n",
    "print(f'AUC-ROC (Ponderaci√≥n): {auc_roc_weighted:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "F1 Score (GridSearchCV): 0.503\n",
      "AUC-ROC (GridSearchCV): 0.780\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Definir los hiperpar√°metros a ajustar\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Par√°metro de regularizaci√≥n\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Algoritmo de optimizaci√≥n\n",
    "    'class_weight': [None, 'balanced']  # Ponderaci√≥n de clases\n",
    "}\n",
    "\n",
    "# Crear el modelo base\n",
    "model = LogisticRegression(random_state=12345)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',  # M√©trica a optimizar (F1 Score)\n",
    "    cv=3,  # N√∫mero de divisiones en la validaci√≥n cruzada\n",
    "    n_jobs=-1  # Usar todos los n√∫cleos del procesador\n",
    ")\n",
    "\n",
    "# Ajustar GridSearchCV a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores hiperpar√°metros encontrados\n",
    "print(\"Mejores hiperpar√°metros:\", grid_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n con el mejor modelo\n",
    "y_pred_grid = grid_search.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_grid = f1_score(y_val, y_pred_grid)\n",
    "auc_roc_grid = roc_auc_score(y_val, grid_search.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (GridSearchCV): {f1_grid:.3f}')\n",
    "print(f'AUC-ROC (GridSearchCV): {auc_roc_grid:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'C': 0.01, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "F1 Score (Sobremuestreo + GridSearchCV): 0.500\n",
      "AUC-ROC (Sobremuestreo + GridSearchCV): 0.780\n"
     ]
    }
   ],
   "source": [
    "#Sobremuestreo + Ajuste hiperparametros\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Funci√≥n para sobremuestrear la clase minoritaria\n",
    "def oversample(X, y):\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    \n",
    "    X_zeros = X[y == 0]\n",
    "    X_ones = X[y == 1]\n",
    "    y_zeros = y[y == 0]\n",
    "    y_ones = y[y == 1]\n",
    "\n",
    "    n_zeros = len(X_zeros)\n",
    "    n_ones = len(X_ones)\n",
    "    repeat_times = n_zeros // n_ones  # N√∫mero de veces que se duplicar√° la clase minoritaria\n",
    "\n",
    "    X_ones_oversampled = pd.concat([X_ones] * repeat_times)\n",
    "    y_ones_oversampled = pd.concat([y_ones] * repeat_times)\n",
    "\n",
    "    X_oversampled = pd.concat([X_zeros, X_ones_oversampled])\n",
    "    y_oversampled = pd.concat([y_zeros, y_ones_oversampled])\n",
    "\n",
    "    return shuffle(X_oversampled, y_oversampled, random_state=12345)\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "# Preprocesamiento\n",
    "data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)  # Eliminar columnas irrelevantes\n",
    "data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)  # Codificar variables categ√≥ricas\n",
    "\n",
    "# Imputar valores nulos en 'Tenure'\n",
    "median_tenure = data['Tenure'].median()\n",
    "data['Tenure'].fillna(median_tenure, inplace=True)\n",
    "\n",
    "# Separar caracter√≠sticas (X) y objetivo (y)\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validaci√≥n y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12345)\n",
    "\n",
    "# Estandarizar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar sobremuestreo\n",
    "X_train_oversampled, y_train_oversampled = oversample(X_train, y_train)\n",
    "\n",
    "# Definir los hiperpar√°metros a ajustar\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Par√°metro de regularizaci√≥n\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Algoritmo de optimizaci√≥n\n",
    "    'class_weight': [None, 'balanced']  # Ponderaci√≥n de clases\n",
    "}\n",
    "\n",
    "# Crear el modelo base\n",
    "model = LogisticRegression(random_state=12345)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',  # M√©trica a optimizar (F1 Score)\n",
    "    cv=3,  # N√∫mero de divisiones en la validaci√≥n cruzada\n",
    "    n_jobs=-1  # Usar todos los n√∫cleos del procesador\n",
    ")\n",
    "\n",
    "# Ajustar GridSearchCV a los datos sobremuestreados\n",
    "grid_search.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Mejores hiperpar√°metros encontrados\n",
    "print(\"Mejores hiperpar√°metros:\", grid_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n con el mejor modelo\n",
    "y_pred_grid = grid_search.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_grid = f1_score(y_val, y_pred_grid)\n",
    "auc_roc_grid = roc_auc_score(y_val, grid_search.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Sobremuestreo + GridSearchCV): {f1_grid:.3f}')\n",
    "print(f'AUC-ROC (Sobremuestreo + GridSearchCV): {auc_roc_grid:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a probar otros tipos de modelos para acercarse a un mejor F1 Score, de minimo 59%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo arbol de decisi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros (√Årbol de Decisi√≥n): {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "F1 Score (√Årbol de Decisi√≥n): 0.486\n",
      "AUC-ROC (√Årbol de Decisi√≥n): 0.676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Definir los hiperpar√°metros a ajustar\n",
    "param_grid_tree = {\n",
    "    'max_depth': [3, 5, 7, 10, None],  # Profundidad m√°xima del √°rbol\n",
    "    'min_samples_split': [2, 5, 10],  # M√≠nimo n√∫mero de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],  # M√≠nimo n√∫mero de muestras en una hoja\n",
    "    'class_weight': [None, 'balanced']  # Ponderaci√≥n de clases\n",
    "}\n",
    "\n",
    "# Crear el modelo base\n",
    "tree_model = DecisionTreeClassifier(random_state=12345)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search_tree = GridSearchCV(\n",
    "    estimator=tree_model,\n",
    "    param_grid=param_grid_tree,\n",
    "    scoring='f1',  # M√©trica a optimizar (F1 Score)\n",
    "    cv=3,  # N√∫mero de divisiones en la validaci√≥n cruzada\n",
    "    n_jobs=-1  # Usar todos los n√∫cleos del procesador\n",
    ")\n",
    "\n",
    "# Ajustar GridSearchCV a los datos sobremuestreados\n",
    "grid_search_tree.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Mejores hiperpar√°metros encontrados\n",
    "print(\"Mejores hiperpar√°metros (√Årbol de Decisi√≥n):\", grid_search_tree.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n con el mejor modelo\n",
    "y_pred_tree = grid_search_tree.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_tree = f1_score(y_val, y_pred_tree)\n",
    "auc_roc_tree = roc_auc_score(y_val, grid_search_tree.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (√Årbol de Decisi√≥n): {f1_tree:.3f}')\n",
    "print(f'AUC-ROC (√Årbol de Decisi√≥n): {auc_roc_tree:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Bosque aleatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con ajuste de hiperparametros y sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros (Bosque Aleatorio): {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score (Bosque Aleatorio): 0.628\n",
      "AUC-ROC (Bosque Aleatorio): 0.850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Definir los hiperpar√°metros a ajustar\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # N√∫mero de √°rboles en el bosque\n",
    "    'max_depth': [3, 5, 7, 10, None],  # Profundidad m√°xima de cada √°rbol\n",
    "    'min_samples_split': [2, 5, 10],  # M√≠nimo n√∫mero de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],  # M√≠nimo n√∫mero de muestras en una hoja\n",
    "    'class_weight': [None, 'balanced']  # Ponderaci√≥n de clases\n",
    "}\n",
    "\n",
    "# Crear el modelo base\n",
    "rf_model = RandomForestClassifier(random_state=12345)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='f1',  # M√©trica a optimizar (F1 Score)\n",
    "    cv=3,  # N√∫mero de divisiones en la validaci√≥n cruzada\n",
    "    n_jobs=-1  # Usar todos los n√∫cleos del procesador\n",
    ")\n",
    "\n",
    "# Ajustar GridSearchCV a los datos sobremuestreados\n",
    "grid_search_rf.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Mejores hiperpar√°metros encontrados\n",
    "print(\"Mejores hiperpar√°metros (Bosque Aleatorio):\", grid_search_rf.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n con el mejor modelo\n",
    "y_pred_rf = grid_search_rf.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_rf = f1_score(y_val, y_pred_rf)\n",
    "auc_roc_rf = roc_auc_score(y_val, grid_search_rf.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Bosque Aleatorio): {f1_rf:.3f}')\n",
    "print(f'AUC-ROC (Bosque Aleatorio): {auc_roc_rf:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Bosque Aleatorio es el que mejores resultados presenta con ajustes de sobremuestreo y en hiperparametros, con un F1 Score de 62.8%, superando el m√≠nimo de 59%.Tambi√©n se encuentra una mejora en el AUC-ROC con un 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de clases despu√©s de submuestreo:\n",
      "0    2794\n",
      "1    1411\n",
      "Name: Exited, dtype: int64\n",
      "F1 Score (Bosque Aleatorio - Submuestreo): 0.632\n",
      "AUC-ROC (Bosque Aleatorio - Submuestreo): 0.854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Funci√≥n para submuestrear la clase mayoritaria\n",
    "def downsample(X, y, fraction):\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    \n",
    "    # Filtrar las clases\n",
    "    X_zeros = X[y == 0]\n",
    "    X_ones = X[y == 1]\n",
    "    y_zeros = y[y == 0]\n",
    "    y_ones = y[y == 1]\n",
    "\n",
    "    # Submuestrear la clase mayoritaria\n",
    "    X_zeros_downsampled = X_zeros.sample(frac=fraction, random_state=12345)\n",
    "    y_zeros_downsampled = y_zeros.sample(frac=fraction, random_state=12345)\n",
    "\n",
    "    # Combinar las clases\n",
    "    X_downsampled = pd.concat([X_zeros_downsampled, X_ones])\n",
    "    y_downsampled = pd.concat([y_zeros_downsampled, y_ones])\n",
    "\n",
    "    # Mezclar los datos\n",
    "    return shuffle(X_downsampled, y_downsampled, random_state=12345)\n",
    "\n",
    "# Aplicar submuestreo\n",
    "fraction = 0.5  # Fracci√≥n de la clase mayoritaria a mantener\n",
    "X_train_downsampled, y_train_downsampled = downsample(X_train, y_train, fraction)\n",
    "\n",
    "# Verificar el nuevo tama√±o de los datos\n",
    "print(\"Distribuci√≥n de clases despu√©s de submuestreo:\")\n",
    "print(pd.Series(y_train_downsampled).value_counts())\n",
    "\n",
    "# Entrenar el modelo de Bosque Aleatorio sin ajuste de hiperpar√°metros\n",
    "rf_model = RandomForestClassifier(random_state=12345, class_weight='balanced')\n",
    "rf_model.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Predecir en el conjunto de validaci√≥n\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Calcular F1 y AUC-ROC\n",
    "f1_rf = f1_score(y_val, y_pred_rf)\n",
    "auc_roc_rf = roc_auc_score(y_val, rf_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f'F1 Score (Bosque Aleatorio - Submuestreo): {f1_rf:.3f}')\n",
    "print(f'AUC-ROC (Bosque Aleatorio - Submuestreo): {auc_roc_rf:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Bosque Aleatorio es el que mejores resultados presenta con ajustes de submuestreo, con un F1 Score de 63.2%, superando el m√≠nimo de 59%.Tambi√©n se encuentra una mejora en el AUC-ROC con un 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4 - Prueba final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral √≥ptimo: 0.530\n",
      "F1 Score (Validaci√≥n - Umbral √ìptimo): 0.637\n",
      "AUC-ROC (Validaci√≥n - Umbral √ìptimo): 0.854\n",
      "F1 Score (Prueba Final - Umbral √ìptimo): 0.593\n",
      "AUC-ROC (Prueba Final - Umbral √ìptimo): 0.853\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo de Bosque Aleatorio sin ajuste de hiperpar√°metros\n",
    "rf_model = RandomForestClassifier(random_state=12345, class_weight='balanced')\n",
    "rf_model.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Obtener las probabilidades de la clase positiva (1) en el conjunto de validaci√≥n\n",
    "y_probs_val = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Encontrar el umbral √≥ptimo que maximiza el F1 Score\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_probs_val)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f'Umbral √≥ptimo: {optimal_threshold:.3f}')\n",
    "\n",
    "# Aplicar el umbral √≥ptimo en el conjunto de validaci√≥n\n",
    "y_pred_val_optimal = (y_probs_val >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calcular F1 y AUC-ROC en el conjunto de validaci√≥n con el umbral √≥ptimo\n",
    "f1_val_optimal = f1_score(y_val, y_pred_val_optimal)\n",
    "auc_roc_val_optimal = roc_auc_score(y_val, y_probs_val)\n",
    "\n",
    "print(f'F1 Score (Validaci√≥n - Umbral √ìptimo): {f1_val_optimal:.3f}')\n",
    "print(f'AUC-ROC (Validaci√≥n - Umbral √ìptimo): {auc_roc_val_optimal:.3f}')\n",
    "\n",
    "# Aplicar el umbral √≥ptimo en el conjunto de prueba\n",
    "y_probs_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_test_optimal = (y_probs_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calcular F1 y AUC-ROC en el conjunto de prueba con el umbral √≥ptimo\n",
    "f1_test_optimal = f1_score(y_test, y_pred_test_optimal)\n",
    "auc_roc_test_optimal = roc_auc_score(y_test, y_probs_test)\n",
    "\n",
    "print(f'F1 Score (Prueba Final - Umbral √ìptimo): {f1_test_optimal:.3f}')\n",
    "print(f'AUC-ROC (Prueba Final - Umbral √ìptimo): {auc_roc_test_optimal:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Excelente trabajo implementado las t√©cnicas para el manejo de datos desbalanceados. Has logrado mejorar el rendimiento de los modelos de clasificaci√≥n que planteaste, Aunque es importante saber que los efectos en el rendimiento depender√° de los modelos utilizados, las t√©cnicas de equilibrio empleadas y de la estructura misma de los datos. En otras circuntancias, los resultados pueden variar, por eso es clave hacer varias pruebas con el fin de identificar la mejor combinaci√≥n o estrateg√≠a que nos de el mejor resultado\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de aplicar diversas mejoras (sobremuestreo, el submuestreo, la ponderaci√≥n de clases, ajuste de umbral de clasificaci√≥n) y a diferentes modelos (regresi√≥n logistica, arbol de decisi√≥n y bosque), se puede concluir que el modelo de Bosque Aleatorio con submuestreo y el umbral ajustado ha demostrado un rendimiento excelente, alcanzando un F1 Score de 59,3% y un AUC-ROC de 85,3% en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comentario del revisor (1ra Iteraci√≥n)</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Tu enfoque en la optimizaci√≥n de modelos mediante t√©cnicas como el sobremuestreo, submuestreo y ajuste de umbrales demuestra un manejo s√≥lido de estrategias para abordar problemas de desbalanceo de clases. Es destacable c√≥mo has evaluado m√∫ltiples algoritmos, identificando que el Bosque Aleatorio, combinado con submuestreo y un umbral ajustado, ofrece el mejor rendimiento. La elecci√≥n de m√©tricas como el F1 Score y el AUC-ROC refleja una comprensi√≥n profunda de la importancia de equilibrar precisi√≥n y recall en contextos desbalanceados. Para futuros trabajos, podr√≠as explorar la interpretabilidad del modelo, utilizando t√©cnicas como la importancia de caracter√≠sticas o SHAP values, lo que podr√≠a proporcionar insights adicionales sobre los factores que m√°s influyen en las predicciones\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 746,
    "start_time": "2025-03-11T14:51:14.984Z"
   },
   {
    "duration": 35,
    "start_time": "2025-03-11T14:51:20.722Z"
   },
   {
    "duration": 418,
    "start_time": "2025-03-11T14:52:39.128Z"
   },
   {
    "duration": 30,
    "start_time": "2025-03-11T14:52:46.798Z"
   },
   {
    "duration": 71,
    "start_time": "2025-03-11T14:52:49.710Z"
   },
   {
    "duration": 77,
    "start_time": "2025-03-11T14:55:48.067Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-11T14:56:26.221Z"
   },
   {
    "duration": 72,
    "start_time": "2025-03-11T14:56:48.081Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-11T15:05:45.075Z"
   },
   {
    "duration": 68,
    "start_time": "2025-03-11T15:05:59.774Z"
   },
   {
    "duration": 65,
    "start_time": "2025-03-11T16:42:00.823Z"
   },
   {
    "duration": 70,
    "start_time": "2025-03-11T16:51:11.069Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-11T16:51:44.554Z"
   },
   {
    "duration": 2,
    "start_time": "2025-03-11T16:52:20.293Z"
   },
   {
    "duration": 86,
    "start_time": "2025-03-11T16:52:31.234Z"
   },
   {
    "duration": 69,
    "start_time": "2025-03-11T16:53:55.493Z"
   },
   {
    "duration": 73,
    "start_time": "2025-03-11T16:57:09.060Z"
   },
   {
    "duration": 71,
    "start_time": "2025-03-11T16:57:21.728Z"
   },
   {
    "duration": 71,
    "start_time": "2025-03-11T16:59:19.325Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-11T16:59:40.195Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-11T17:02:17.268Z"
   },
   {
    "duration": 46,
    "start_time": "2025-03-11T17:02:18.635Z"
   },
   {
    "duration": 26,
    "start_time": "2025-03-11T17:02:23.272Z"
   },
   {
    "duration": 61,
    "start_time": "2025-03-11T17:06:11.600Z"
   },
   {
    "duration": 627,
    "start_time": "2025-03-11T17:06:51.428Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-11T17:09:30.973Z"
   },
   {
    "duration": 13,
    "start_time": "2025-03-11T17:09:56.039Z"
   },
   {
    "duration": 129,
    "start_time": "2025-03-11T17:13:53.666Z"
   },
   {
    "duration": 22,
    "start_time": "2025-03-11T17:13:59.911Z"
   },
   {
    "duration": 150,
    "start_time": "2025-03-11T17:18:26.560Z"
   },
   {
    "duration": 11,
    "start_time": "2025-03-11T17:20:49.256Z"
   },
   {
    "duration": 20,
    "start_time": "2025-03-11T17:23:03.863Z"
   },
   {
    "duration": 22,
    "start_time": "2025-03-11T17:29:22.969Z"
   },
   {
    "duration": 9071,
    "start_time": "2025-03-11T17:30:36.510Z"
   },
   {
    "duration": 771,
    "start_time": "2025-03-11T18:26:04.492Z"
   },
   {
    "duration": 61,
    "start_time": "2025-03-11T18:26:05.266Z"
   },
   {
    "duration": 24,
    "start_time": "2025-03-11T18:26:05.329Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-11T18:26:05.356Z"
   },
   {
    "duration": 27,
    "start_time": "2025-03-11T18:26:05.363Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-11T18:26:05.425Z"
   },
   {
    "duration": 87,
    "start_time": "2025-03-11T18:26:05.439Z"
   },
   {
    "duration": 1648,
    "start_time": "2025-03-11T18:26:05.528Z"
   },
   {
    "duration": 28,
    "start_time": "2025-03-11T18:26:57.866Z"
   },
   {
    "duration": 26,
    "start_time": "2025-03-11T18:28:41.089Z"
   },
   {
    "duration": 55,
    "start_time": "2025-03-11T18:33:14.994Z"
   },
   {
    "duration": 2152,
    "start_time": "2025-03-11T18:38:48.496Z"
   },
   {
    "duration": 2448,
    "start_time": "2025-03-11T18:42:58.797Z"
   },
   {
    "duration": 5549,
    "start_time": "2025-03-11T18:45:37.415Z"
   },
   {
    "duration": 405747,
    "start_time": "2025-03-11T18:46:40.777Z"
   },
   {
    "duration": 1899,
    "start_time": "2025-03-11T18:59:25.518Z"
   },
   {
    "duration": 154,
    "start_time": "2025-03-11T19:02:46.816Z"
   },
   {
    "duration": 1987,
    "start_time": "2025-03-11T19:05:58.320Z"
   },
   {
    "duration": 40,
    "start_time": "2025-03-11T19:07:51.812Z"
   },
   {
    "duration": 23282,
    "start_time": "2025-03-11T19:09:09.587Z"
   },
   {
    "duration": 436,
    "start_time": "2025-03-11T19:25:21.291Z"
   },
   {
    "duration": 457,
    "start_time": "2025-03-11T19:25:58.929Z"
   },
   {
    "duration": 465,
    "start_time": "2025-03-11T19:29:42.959Z"
   },
   {
    "duration": 463,
    "start_time": "2025-03-11T19:31:23.446Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-11T19:35:57.585Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
